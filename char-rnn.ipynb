{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8', 'j', 'z', 'I', 'v', '–', ')', 'B', 'H', 'F', '9', '#', 'o', 'a', ':', 'W', '[', '0', 'd', ' ', 'O', 'r', 'D', 'J', 'E', '5', 'M', '4', 'p', 'b', 'g', 'u', 't', 'q', 'V', 'e', 'k', 'K', 'P', 'G', 'm', ']', '-', 's', 'x', 'N', 'n', 'l', 'C', 'T', ',', 'L', 'i', 'f', '.', ';', 'S', \"'\", '2', 'h', 'c', '1', '6', '(', 'y', '7', 'R', 'w', 'A', '3']\n",
      "data has 8116 characters, 70 unique.\n",
      "{'8': 0, ';': 55, 'j': 1, 'c': 60, 'z': 2, '6': 62, 'I': 3, 'v': 4, '–': 5, ')': 6, ' ': 19, 'F': 9, 'H': 8, '9': 10, '#': 11, 'r': 21, 'o': 12, 'a': 13, ':': 14, 'W': 15, '0': 17, 'd': 18, '5': 25, 'O': 20, 'D': 22, 'E': 24, 'J': 23, 'B': 7, 'M': 26, '4': 27, 'p': 28, 'g': 30, 'f': 53, 'q': 33, 't': 32, 'e': 35, 'k': 36, 'K': 37, 'u': 31, 'P': 38, 'G': 39, 'm': 40, ']': 41, '-': 42, 's': 43, 'x': 44, 'N': 45, 'n': 46, 'R': 66, 'l': 47, 'C': 48, 'T': 49, 'b': 29, 'L': 51, 'i': 52, '.': 54, 'S': 56, \"'\": 57, 'h': 59, '[': 16, '1': 61, '(': 63, 'V': 34, '2': 58, 'y': 64, '7': 65, ',': 50, 'w': 67, 'A': 68, '3': 69}\n",
      "{0: '8', 1: 'j', 2: 'z', 3: 'I', 4: 'v', 5: '–', 6: ')', 7: 'B', 8: 'H', 9: 'F', 10: '9', 11: '#', 12: 'o', 13: 'a', 14: ':', 15: 'W', 16: '[', 17: '0', 18: 'd', 19: ' ', 20: 'O', 21: 'r', 22: 'D', 23: 'J', 24: 'E', 25: '5', 26: 'M', 27: '4', 28: 'p', 29: 'b', 30: 'g', 31: 'u', 32: 't', 33: 'q', 34: 'V', 35: 'e', 36: 'k', 37: 'K', 38: 'P', 39: 'G', 40: 'm', 41: ']', 42: '-', 43: 's', 44: 'x', 45: 'N', 46: 'n', 47: 'l', 48: 'C', 49: 'T', 50: ',', 51: 'L', 52: 'i', 53: 'f', 54: '.', 55: ';', 56: 'S', 57: \"'\", 58: '2', 59: 'h', 60: 'c', 61: '1', 62: '6', 63: '(', 64: 'y', 65: '7', 66: 'R', 67: 'w', 68: 'A', 69: '3'}\n"
     ]
    }
   ],
   "source": [
    "data = \"A gang of criminals rob a Gotham City mob bank, murdering each other until only the mastermind remains: the Joker, who escapes with the money. Batman, District Attorney Harvey Dent and Lieutenant Jim Gordon form an alliance to rid Gotham of organized crime. Bruce Wayne believes that, with Dent as Gotham's protector, he can retire from being Batman and lead a normal life with Rachel Dawes – even though she and Dent are dating. Mob bosses Sal Maroni, Gambol, and the Chechen hold a videoconference with their corrupt accountant, Lau, who has taken their funds for safekeeping and fled to Hong Kong. The Joker interrupts and warns them that Batman has no jurisdiction and is unhindered by the law, offering to kill him in exchange for half of their money. After Gambol puts a bounty on his head, the Joker kills Gambol and takes over his gang. The mob ultimately decides to take the Joker up on his offer. Batman finds Lau in Hong Kong and brings him back to Gotham to testify, allowing Dent to apprehend the entire mob. The Joker threatens to keep killing people unless Batman reveals his identity, and starts by murdering Police Commissioner Gillian B. Loeb and the judge presiding over the mob trial. The Joker also tries to kill Mayor Anthony Garcia, but Gordon sacrifices himself to stop the assassination. Dent learns that Rachel is the next target. Bruce decides to reveal his secret identity. Before he can, however, Dent falsely announces that he is Batman. Dent is taken into protective custody, but the Joker appears and attacks the convoy. Batman comes to Dent's rescue and Gordon, who faked his death, apprehends the Joker, securing a promotion to Commissioner. Rachel and Dent are escorted away by detectives on Maroni's payroll; Gordon later learns that they never arrived home. Batman interrogates the Joker, who reveals that they have been trapped in separate locations rigged with explosives. Batman races to save Rachel, while Gordon attempts to rescue Dent. Batman arrives at the building, but realizes that the Joker sent him to Dent's location instead. Both buildings explode, killing Rachel and disfiguring half of Dent's face. The Joker escapes with Lau, whom he later kills along with the Chechen. Coleman Reese, an accountant at Wayne Enterprises, deduces that Bruce is Batman and tries to go public with the information. Not wanting Reese's revelation to interfere with his plans, the Joker threatens to destroy a hospital unless someone kills Reese within an hour. Gordon orders the evacuation of all the hospitals in Gotham and goes to secure Reese. The Joker gives Dent a gun and convinces him to seek revenge for Rachel's death, then destroys the hospital and escapes with a busload of hostages. Dent goes on a killing spree, deciding the fates of people he holds responsible for Rachel's death by flipping his lucky coin. After announcing Gotham will be subject to his rule come nightfall, the Joker rigs two evacuating ferries with explosives; one containing civilians and the other containing prisoners. He says that he will blow them both up by midnight, but will let one live if its passengers (who have been supplied the trigger to the other boat's explosives) blow up the other. Batman finds the Joker by using a sonar device that spies on the entire city, with the reluctant help of Lucius Fox. Both the civilians and the prisoners refuse to kill each other, while Batman apprehends the Joker after a brief fight. Before the police arrive to take the Joker into custody, he gloats that Gotham's citizens will lose hope once Dent's rampage becomes public knowledge. Gordon and Batman arrive at the building where Rachel perished. Dent shoots Batman, spares himself, and threatens to kill Gordon's son, claiming that Gordon's negligence is responsible for Rachel's death. Before he can flip for the boy, Batman, who was wearing body armor, tackles Dent off the building to his death. Batman persuades Gordon to hold him responsible for the killing spree to preserve Dent's heroic image. As the police launch a manhunt for Batman, Gordon destroys the Bat-signal, Fox watches as the sonar device self-destructs, and Alfred Pennyworth burns a letter from Rachel saying she planned to marry Dent. Before the release of Batman Begins, screenwriter David S. Goyer wrote a treatment for two sequels which introduced the Joker and Harvey Dent. His original intent was for the Joker to scar Dent during the Joker's trial in the third film, turning Dent into Two-Face. Goyer, who penned the first draft of the film, cited the DC Comics 13-issue comic book limited series Batman: The Long Halloween as the major influence on his storyline. According to veteran Batman artist Neal Adams, he met with David Goyer in Los Angeles, and the story would eventually look to Adams and writer Denny O'Neil's 1971 story The Joker's Five-Way Revenge that appeared in Batman #251, in which O'Neil and Adams re-introduced the Joker.[61] While initially uncertain of whether or not he would return to direct the sequel, Nolan did want to reinterpret the Joker on screen. On July 31, 2006, Warner Bros. officially announced initiation of production for the sequel to Batman Begins titled The Dark Knight;[62] it is the first live-action Batman film without the word  Batman  in its title, which Bale noted as signaling that  this take on Batman of mine and Chris' is very different from any of the others . After much research, Nolan's brother and co-writer, Jonathan, suggested the Joker's first two appearances, published in the first issue of Batman (1940), as the crucial influences. Christopher had Jonathan watch Fritz Lang's 1933 crime film The Testament of Dr. Mabuse prior to writing the Joker, with the Joker resembling Mabuse's characteristics. Christopher Nolan referred to Lang's film as  essential research for anyone attempting to write a supervillain . Jerry Robinson, one of the Joker's co-creators, was consulted on the character's portrayal.[68] Nolan decided to avoid divulging an in-depth origin story for the Joker, and instead portray his rise to power so as to not diminish the threat he poses, explaining to MTV News,  the Joker we meet in The Dark Knight is fully formed ... To me, the Joker is an absolute. There are no shades of gray to him – maybe shades of purple. He's unbelievably dark. He bursts in just as he did in the comics.  Nolan reiterated to IGN,  We never wanted to do an origin story for the Joker in this film , because  the arc of the story is much more Harvey Dent's; the Joker is presented as an absolute. It's a very thrilling element in the film, and a very important element, but we wanted to deal with the rise of the Joker, not the origin of the Joker.  Nolan suggested Batman: The Killing Joke influenced a section of the Joker's dialogue in the film, in which he says that anyone can become like him given the right circumstances.[70] Nolan also cited Heat as  sort of an inspiration  for his aim  to tell a very large, city story or the story of a city :  If you want to take on Gotham, you want to give Gotham a kind of weight and breadth and depth in there. So you wind up dealing with the political figures, the media figures. That's part of the whole fabric of how a city is bound together . According to Nolan, an important theme of the sequel is  escalation , extending the ending of Batman Begins, noting  things having to get worse before they get better . While indicating The Dark Knight would continue the themes of Batman Begins, including justice vs. revenge and Bruce Wayne's issues with his father, Nolan emphasized the sequel would also portray Wayne more as a detective, an aspect of his character not fully developed in Batman Begins. Nolan described the friendly rivalry between Bruce Wayne and Harvey Dent as the  backbone  of the film. He also chose to compress the overall storyline, allowing Dent to become Two-Face in The Dark Knight, thus giving the film an emotional arc the unsympathetic Joker could not offer. Nolan acknowledged the title was not only a reference to Batman, but also the fallen  white knight  Harvey Dent.\"\n",
    "chars = list(set(data))\n",
    "print(chars)\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "# print(data_size)\n",
    "# print(vocab_size)\n",
    "print(\"data has \"+str(data_size)+\" characters, \"+str(vocab_size)+\" unique.\")\n",
    "\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "print(char_to_ix)\n",
    "print(ix_to_char)\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zv[rv5\n",
      "[7, 13, 32, 40, 13, 46]\n"
     ]
    }
   ],
   "source": [
    "def array2text(arr):\n",
    "    text=\"\"\n",
    "    for i in arr:\n",
    "        text+=ix_to_char[i]\n",
    "    return text\n",
    "\n",
    "print(array2text([2, 4, 16, 21, 4, 25]))\n",
    "def text2array(string):\n",
    "    arr=[]\n",
    "    for i in string:\n",
    "        arr.append(char_to_ix[i])\n",
    "    return arr\n",
    "\n",
    "print(text2array(\"Batman\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 106.14736424814065\n",
      "iter 100, loss: 105.09035661338844\n",
      "iter 200, loss: 101.98857543544513\n",
      "iter 300, loss: 98.99517212266446\n",
      "iter 400, loss: 95.97775253759515\n",
      "iter 500, loss: 93.07922164082\n",
      "iter 600, loss: 90.59928673683659\n",
      "iter 700, loss: 88.1830286772209\n",
      "iter 800, loss: 85.72677095115213\n",
      "iter 900, loss: 83.74674636590389\n",
      "iter 1000, loss: 81.78151786243228\n",
      "iter 1100, loss: 79.8547515382697\n",
      "iter 1200, loss: 78.29539871059919\n",
      "iter 1300, loss: 76.89018106344525\n",
      "iter 1400, loss: 75.32021072869374\n",
      "iter 1500, loss: 73.94967418109951\n",
      "iter 1600, loss: 72.80736439071204\n",
      "iter 1700, loss: 71.58980434879604\n",
      "iter 1800, loss: 70.2926110136401\n",
      "iter 1900, loss: 69.36075240180249\n",
      "iter 2000, loss: 68.46601955823522\n",
      "iter 2100, loss: 67.28047860602457\n",
      "iter 2200, loss: 66.51909090693044\n",
      "iter 2300, loss: 65.76473937862374\n",
      "iter 2400, loss: 64.81702500933618\n",
      "iter 2500, loss: 64.08364981972697\n",
      "iter 2600, loss: 63.50004410805592\n",
      "iter 2700, loss: 62.73192533211291\n",
      "iter 2800, loss: 62.0797632583403\n",
      "iter 2900, loss: 61.5650965613082\n",
      "iter 3000, loss: 61.01515931643383\n",
      "iter 3100, loss: 60.39129634114871\n",
      "iter 3200, loss: 59.95482501058525\n",
      "iter 3300, loss: 59.5923511456616\n",
      "iter 3400, loss: 58.973692539510814\n",
      "iter 3500, loss: 58.62513642439864\n",
      "iter 3600, loss: 58.30596824395931\n",
      "iter 3700, loss: 57.81139835916035\n",
      "iter 3800, loss: 57.461484448186596\n",
      "iter 3900, loss: 57.19323367200805\n",
      "iter 4000, loss: 56.77989036136449\n",
      "iter 4100, loss: 56.42580384375944\n",
      "iter 4200, loss: 56.15793249727503\n",
      "iter 4300, loss: 55.86025827178982\n",
      "iter 4400, loss: 55.532736634919765\n",
      "iter 4500, loss: 55.27552399157825\n",
      "iter 4600, loss: 55.1172490465615\n",
      "iter 4700, loss: 54.74987047626539\n",
      "iter 4800, loss: 54.521671604340405\n",
      "iter 4900, loss: 54.38646603941052\n",
      "iter 5000, loss: 54.05052324402338\n",
      "iter 5100, loss: 53.849678511991144\n",
      "iter 5200, loss: 53.6945020827774\n",
      "iter 5300, loss: 53.469058130074785\n",
      "iter 5400, loss: 53.21384532314458\n",
      "iter 5500, loss: 53.09429145100332\n",
      "iter 5600, loss: 52.88902476705549\n",
      "iter 5700, loss: 52.7151973464332\n",
      "iter 5800, loss: 52.5372704939287\n",
      "iter 5900, loss: 52.455570029413934\n",
      "iter 6000, loss: 52.25334500962484\n",
      "iter 6100, loss: 52.00760015969737\n",
      "iter 6200, loss: 51.99593078067997\n",
      "iter 6300, loss: 51.76208631855167\n",
      "iter 6400, loss: 51.58817617303067\n",
      "iter 6500, loss: 51.548943078711204\n",
      "iter 6600, loss: 51.372725129008806\n",
      "iter 6700, loss: 51.18537519047815\n",
      "iter 6800, loss: 51.11031943883309\n",
      "iter 6900, loss: 50.92620210503417\n",
      "iter 7000, loss: 50.827059955409275\n",
      "iter 7100, loss: 50.69051615284163\n",
      "iter 7200, loss: 50.634046260115234\n",
      "iter 7300, loss: 50.50657545249982\n",
      "iter 7400, loss: 50.29166594341471\n",
      "iter 7500, loss: 50.3201511730076\n",
      "iter 7600, loss: 50.11476201417462\n",
      "iter 7700, loss: 50.00008337243325\n",
      "iter 7800, loss: 50.00409900860202\n",
      "iter 7900, loss: 49.87277133339223\n",
      "iter 8000, loss: 49.680502167106546\n",
      "iter 8100, loss: 49.648302815359465\n",
      "iter 8200, loss: 49.50269170264329\n",
      "iter 8300, loss: 49.40998931905212\n",
      "iter 8400, loss: 49.289762428950226\n",
      "iter 8500, loss: 49.26222885456092\n",
      "iter 8600, loss: 49.1387828731485\n",
      "iter 8700, loss: 48.995581134813705\n",
      "iter 8800, loss: 49.042057140650265\n",
      "iter 8900, loss: 48.85700924247472\n",
      "iter 9000, loss: 48.737248344655754\n",
      "iter 9100, loss: 48.738971894040354\n",
      "iter 9200, loss: 48.62404899993883\n",
      "iter 9300, loss: 48.41364543929058\n",
      "iter 9400, loss: 48.4551184804259\n",
      "iter 9500, loss: 48.34016499689603\n",
      "iter 9600, loss: 48.229987749920866\n",
      "iter 9700, loss: 48.13448885710019\n",
      "iter 9800, loss: 48.12129251361247\n",
      "iter 9900, loss: 48.004927897705194\n",
      "iter 10000, loss: 47.86880026377298\n",
      "iter 10100, loss: 47.92609635617852\n",
      "iter 10200, loss: 47.74901108282227\n",
      "iter 10300, loss: 47.642899162943436\n",
      "iter 10400, loss: 47.63871904598629\n",
      "iter 10500, loss: 47.512159405129125\n",
      "iter 10600, loss: 47.32186328327897\n",
      "iter 10700, loss: 47.33206718850058\n",
      "iter 10800, loss: 47.248584499259536\n",
      "iter 10900, loss: 47.132323813826766\n",
      "iter 11000, loss: 47.02959400232112\n",
      "iter 11100, loss: 47.0073250578253\n",
      "iter 11200, loss: 46.928346124855686\n",
      "iter 11300, loss: 46.81138636096278\n",
      "iter 11400, loss: 46.85585669069722\n",
      "iter 11500, loss: 46.71403799658212\n",
      "iter 11600, loss: 46.62601190952195\n",
      "iter 11700, loss: 46.65085039431307\n",
      "iter 11800, loss: 46.557343760566695\n",
      "iter 11900, loss: 46.40047148317902\n",
      "iter 12000, loss: 46.40020610171634\n",
      "iter 12100, loss: 46.3329640808534\n",
      "iter 12200, loss: 46.231631839399434\n",
      "iter 12300, loss: 46.15623796727518\n",
      "iter 12400, loss: 46.11434546176527\n",
      "iter 12500, loss: 46.05909263516219\n",
      "iter 12600, loss: 45.95089603725628\n",
      "iter 12700, loss: 45.979350817482704\n",
      "iter 12800, loss: 45.8657814391317\n",
      "iter 12900, loss: 45.75304275898246\n",
      "iter 13000, loss: 45.77310779297363\n",
      "iter 13100, loss: 45.66155873566111\n",
      "iter 13200, loss: 45.53047969015335\n",
      "iter 13300, loss: 45.549617826846756\n",
      "iter 13400, loss: 45.525958692595495\n",
      "iter 13500, loss: 45.375991726439175\n",
      "iter 13600, loss: 45.341663304773\n",
      "iter 13700, loss: 45.3306008405202\n",
      "iter 13800, loss: 45.303770073751764\n",
      "iter 13900, loss: 45.17375278905665\n",
      "iter 14000, loss: 45.187323980580985\n",
      "iter 14100, loss: 45.13260919393438\n",
      "iter 14200, loss: 44.93495039041539\n",
      "iter 14300, loss: 44.98786988473747\n",
      "iter 14400, loss: 44.88751232901944\n",
      "iter 14500, loss: 44.76767675107086\n",
      "iter 14600, loss: 44.800535099198356\n",
      "iter 14700, loss: 44.74351156408144\n",
      "iter 14800, loss: 44.58800684891085\n",
      "iter 14900, loss: 44.58123304405107\n",
      "iter 15000, loss: 44.47804925585821\n",
      "iter 15100, loss: 44.43329711594911\n",
      "iter 15200, loss: 44.35217341932111\n",
      "iter 15300, loss: 44.36364316472099\n",
      "iter 15400, loss: 44.30797322766215\n",
      "iter 15500, loss: 44.10613607684329\n",
      "iter 15600, loss: 44.18497266224548\n",
      "iter 15700, loss: 44.07200014536984\n",
      "iter 15800, loss: 43.92262067567844\n",
      "iter 15900, loss: 43.97389632814797\n",
      "iter 16000, loss: 43.921515281641355\n",
      "iter 16100, loss: 43.76412684210357\n",
      "iter 16200, loss: 43.740840562292846\n",
      "iter 16300, loss: 43.68006861154508\n",
      "iter 16400, loss: 43.63603411058045\n",
      "iter 16500, loss: 43.52914818095898\n",
      "iter 16600, loss: 43.5696245104858\n",
      "iter 16700, loss: 43.514882756221915\n",
      "iter 16800, loss: 43.317722350282736\n",
      "iter 16900, loss: 43.43252506747132\n",
      "iter 17000, loss: 43.33186501523416\n",
      "iter 17100, loss: 43.18760998943251\n",
      "iter 17200, loss: 43.250650095620884\n",
      "iter 17300, loss: 43.20772929963181\n",
      "iter 17400, loss: 43.04837836920383\n",
      "iter 17500, loss: 43.11471969253834\n",
      "iter 17600, loss: 43.07107265528315\n",
      "iter 17700, loss: 43.01481336206347\n",
      "iter 17800, loss: 42.94589617185404\n",
      "iter 17900, loss: 42.96963138534086\n",
      "iter 18000, loss: 42.920289294350525\n",
      "iter 18100, loss: 42.74702800213654\n",
      "iter 18200, loss: 42.8420845747345\n",
      "iter 18300, loss: 42.7357630636773\n",
      "iter 18400, loss: 42.62990973588519\n",
      "iter 18500, loss: 42.669559956238885\n",
      "iter 18600, loss: 42.59136495304076\n",
      "iter 18700, loss: 42.41783837137709\n",
      "iter 18800, loss: 42.430703367561094\n",
      "iter 18900, loss: 42.390845767497005\n",
      "iter 19000, loss: 42.288398319127936\n",
      "iter 19100, loss: 42.19017761673246\n",
      "iter 19200, loss: 42.25230503874988\n",
      "iter 19300, loss: 42.20247136818686\n",
      "iter 19400, loss: 42.086228332788906\n",
      "iter 19500, loss: 42.1752792767809\n",
      "iter 19600, loss: 42.0809162230077\n",
      "iter 19700, loss: 41.95825362461181\n",
      "iter 19800, loss: 41.99168820752795\n",
      "iter 19900, loss: 41.93385660439506\n",
      "iter 20000, loss: 41.783319594964425\n",
      "iter 20100, loss: 41.8265176587227\n",
      "iter 20200, loss: 41.83713347559666\n",
      "iter 20300, loss: 41.7226969131253\n",
      "iter 20400, loss: 41.69556142736372\n",
      "iter 20500, loss: 41.69976486511045\n",
      "iter 20600, loss: 41.69182039577646\n",
      "iter 20700, loss: 41.59729177218156\n",
      "iter 20800, loss: 41.66607302895948\n",
      "iter 20900, loss: 41.616440910056305\n",
      "iter 21000, loss: 41.49989509602524\n",
      "iter 21100, loss: 41.583354971074435\n",
      "iter 21200, loss: 41.516032113293846\n",
      "iter 21300, loss: 41.402558854352726\n",
      "iter 21400, loss: 41.428245137394946\n",
      "iter 21500, loss: 41.47058265980592\n",
      "iter 21600, loss: 41.36014040108096\n",
      "iter 21700, loss: 41.303958079984916\n",
      "iter 21800, loss: 41.27182127502348\n",
      "iter 21900, loss: 41.24417304946754\n",
      "iter 22000, loss: 41.120013290981184\n",
      "iter 22100, loss: 41.16574704931392\n",
      "iter 22200, loss: 41.12458352419284\n",
      "iter 22300, loss: 40.954118212609515\n",
      "iter 22400, loss: 41.00727990899238\n",
      "iter 22500, loss: 40.941436492655725\n",
      "iter 22600, loss: 40.850791681313254\n",
      "iter 22700, loss: 40.903725973851095\n",
      "iter 22800, loss: 40.898935230781454\n",
      "iter 22900, loss: 40.798658400013466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 23000, loss: 40.75635567055962\n",
      "iter 23100, loss: 40.73782095309175\n",
      "iter 23200, loss: 40.76079497806088\n",
      "iter 23300, loss: 40.694546485688726\n",
      "iter 23400, loss: 40.75458869093272\n",
      "iter 23500, loss: 40.70659535101307\n",
      "iter 23600, loss: 40.494702757019546\n",
      "iter 23700, loss: 40.60378154643465\n",
      "iter 23800, loss: 40.5176352704826\n",
      "iter 23900, loss: 40.382864303765416\n",
      "iter 24000, loss: 40.48609412739141\n",
      "iter 24100, loss: 40.485187828883625\n",
      "iter 24200, loss: 40.3033792080996\n",
      "iter 24300, loss: 40.29731557260831\n",
      "iter 24400, loss: 40.289857195549054\n",
      "iter 24500, loss: 40.2261995721751\n",
      "iter 24600, loss: 40.110337171675276\n"
     ]
    }
   ],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  if len(inputs)!=len(targets):\n",
    "      targets.append(0)\n",
    "#   print(len(inputs))\n",
    "#   print(len(targets))\n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "#     print(\"ps\")\n",
    "#     print(ps[t])\n",
    "#     print(\"target\")\n",
    "#     print(targets[t])\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  \"\"\"\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []\n",
    "  for t in range(n):\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  return ixes\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while smooth_loss>=0.005:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "#   print(array2text(inputs)+\" with length \"+str(len(inputs)))\n",
    "#   print(array2text(targets)+\" with length \"+str(len(targets)))\n",
    "  # sample from the model now and then\n",
    "#   if n % 100 == 0:\n",
    "#     sample_ix = sample(hprev, inputs[0], 200)\n",
    "#     txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "#     print(\"----\\n \"+txt+\" \\n----\")\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % 100 == 0: print(\"iter \"+str(n)+\", loss: \"+str(smooth_loss)) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inputs,hprev):\n",
    "#   print(inputs)\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  predicted=-1\n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t])\n",
    "    predicted = np.argmax(ps[t])\n",
    "#     print(predicted)\n",
    "  print(ix_to_char[predicted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "input = \"Darkestart\" ##Batman ##[18,37,34,36] ##Nola\n",
    "hprev = np.zeros((hidden_size,1))\n",
    "predict(text2array(input),hprev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
